{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx2awyMRyqcE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK7qv_9_y0j2"
      },
      "source": [
        "### 검색"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFZkt_Qdy2n8",
        "outputId": "19e723ef-b6d5-43b2-bc79-7be3f6e3c522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ax2q6ZVsy3OL"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import json\n",
        "import  torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import io\n",
        "import requests\n",
        "from PIL import Image\n",
        "import PIL\n",
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "drvPEcIWFF1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyvLmJuWy3QJ"
      },
      "outputs": [],
      "source": [
        "# 임베딩 CLIP 모델 정보\n",
        "model_name = \"openai/clip-vit-large-patch14\"\n",
        "clip_model = CLIPModel.from_pretrained(model_name)\n",
        "processor = CLIPProcessor.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# text 임베딩 벡터 반환\n",
        "def encode_text(text):\n",
        "    inputs = processor(text=[text], return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        text_features = clip_model.get_text_features(**inputs)\n",
        "\n",
        "    text_features = torch.nn.functional.normalize(text_features, p=2, dim=-1) #정규화\n",
        "\n",
        "    return text_features.cpu().numpy().astype(\"float32\")\n",
        "\n",
        "# 검색\n",
        "def search_similar_images(text, image_index, top_k=5):\n",
        "    text_vec = encode_text(text)\n",
        "    distances, indices = image_index.search(text_vec, top_k)\n",
        "\n",
        "    # 개별 벡터 가져오기\n",
        "    image_embed_vectors = []\n",
        "    for i in indices[0]:  # 이차원 배열이므로 첫 번째 row에서 가져오기\n",
        "      vec = np.zeros((image_index.d,), dtype=np.float32)  # 빈 벡터 생성\n",
        "      image_index.reconstruct(int(i), vec)  # 특정 ID의 벡터 복원\n",
        "      image_embed_vectors.append(vec) # 저장\n",
        "\n",
        "    image_embed = torch.tensor(np.array(image_embed_vectors), dtype=torch.float32)\n",
        "\n",
        "\n",
        "    return image_embed, distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUI5gK-_y3SZ"
      },
      "outputs": [],
      "source": [
        "# 이미지 입력 받기\n",
        "\n",
        "upload_dir = \"/content/uploads\"\n",
        "os.makedirs(upload_dir, exist_ok=True)\n",
        "\n",
        "# 업로드된 이미지 경로 저장\n",
        "uploaded_image_paths = []\n",
        "\n",
        "# Colab에서 이미지 파일 업로드 및 저장\n",
        "def upload_images():\n",
        "    uploaded_files = files.upload()\n",
        "    for file_name in uploaded_files.keys():\n",
        "        file_path = os.path.join(\"/content\", file_name)  # 기본 업로드 경로\n",
        "        move_path = os.path.join(upload_dir, file_name)  # 사용자 지정 디렉토리\n",
        "\n",
        "        # 파일을 원하는 디렉토리로 이동\n",
        "        os.rename(file_path, move_path)\n",
        "\n",
        "        uploaded_image_paths.append(move_path)\n",
        "        print(f\"save image : {move_path}\")\n",
        "\n",
        "\n",
        "# 버튼 설정\n",
        "upload_button = widgets.Button(description=\"이미지 업로드\")\n",
        "\n",
        "# 클릭 시 실행\n",
        "def on_upload_clicked(b):\n",
        "    upload_images()\n",
        "\n",
        "\n",
        "upload_button.on_click(on_upload_clicked)\n",
        "\n",
        "# 출력\n",
        "display(upload_button)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vB1OMACy3UX"
      },
      "outputs": [],
      "source": [
        "# DB 설정\n",
        "#faiss_index_filename = \"/content/faiss_index_vitL14.index\"  # FAISS 벡터 DB 파일 경로 # 풍경 이미지\n",
        "faiss_index_filename = \"/content/faiss_image_v14.index\"  # FAISS 벡터 DB 파일 경로 # 강아지, 고양이 이미지\n",
        "\n",
        "\n",
        "# FAISS 인덱스 로드\n",
        "db_index = faiss.read_index(faiss_index_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sZf-o0SzIDI"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 텍스트 입력\n",
        "query_text = \"a peaceful village where a big size cat roams, a photo realistic\"\n",
        "\n",
        "# input 이미지 임베딩 + 정규화\n",
        "\n",
        "input_images = [Image.open(path).convert(\"RGB\") for path in uploaded_image_paths] # 입력 이미지 리스트\n",
        "\n",
        "\n",
        "top_k = 3 # 최근접 이웃 개수\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uswJm8wzMA2"
      },
      "outputs": [],
      "source": [
        "# 유사한 이미지 벡터 검색\n",
        "similar_images_emb, distances = search_similar_images(query_text, db_index, top_k=top_k)\n",
        "\n",
        "\n",
        "# top_k 이미지에 대한 가중치 계산\n",
        "\n",
        "# 해당 부분 설정은 자유 -> 다양하게 바꿀 수 있음\n",
        "weights = np.array(distances[0]) # 거리 저장\n",
        "\n",
        "# 정규화 (합이 1이 되도록)\n",
        "normalized_weights = weights / np.sum(weights)\n",
        "top_k_w = normalized_weights.tolist() # 가중치를 list 타입으로 받음\n",
        "print(top_k_w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUwTZ_ZkzkvH"
      },
      "source": [
        "### 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACwSn9qlzmiR"
      },
      "outputs": [],
      "source": [
        "### 라이브러리 파일 교체\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# 교체할 파일 경로\n",
        "file_path = \"/usr/local/lib/python3.11/dist-packages/diffusers/pipelines/kandinsky/pipeline_kandinsky_prior.py\"\n",
        "\n",
        "# 파일 업로드\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "# 업로드된 파일 처리\n",
        "for file_name in uploaded_files.keys():\n",
        "    # 업로드된 파일이 Python 파일인지 확인\n",
        "    if file_name.endswith(\".py\"):\n",
        "        upload_file_path = os.path.join(\"/content\", file_name)  # Colab 기본 업로드 경로\n",
        "\n",
        "        # 파일을 diffusers 경로로 이동하여 덮어쓰기\n",
        "        os.rename(upload_file_path, file_path)\n",
        "\n",
        "        print(f\"업로드 완료\")\n",
        "    else:\n",
        "        print(f\"ERROR\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e2ro2N1znOo"
      },
      "outputs": [],
      "source": [
        "from diffusers import KandinskyPriorPipeline, KandinskyPipeline\n",
        "\n",
        "# 모델 불러오기\n",
        "pipe_prior = KandinskyPriorPipeline.from_pretrained(\n",
        "    \"kandinsky-community/kandinsky-2-1-prior\", torch_dtype=torch.float16\n",
        ")\n",
        "pipe_prior.to(\"cuda\")\n",
        "\n",
        "\n",
        "pipe = KandinskyPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16)\n",
        "pipe.to(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJwG-f0AznQ7"
      },
      "outputs": [],
      "source": [
        "# 텍스트 가중치\n",
        "query_text_w = 0.3\n",
        "cond_image_w = 0.6\n",
        "# 생성 이미지 개수\n",
        "num_images_per_prompt = 2\n",
        "\n",
        "# image_text 컨디션 리스트\n",
        "images_texts = [query_text] + input_images\n",
        "\n",
        "# 가중치 설정\n",
        "weights = [query_text_w] +[cond_image_w/(len(input_images)) for _ in input_images]\n",
        "# 이 뒷부분 [cond_image_w/(len(input_images)) for _ in input_images] 영역은\n",
        "# 이미지 개수에 맞게 직접 설정 가능 [0.3, 0.5] 이런 식\n",
        "\n",
        "prompt = query_text\n",
        "prior_out = pipe_prior.interpolate(images_texts,\n",
        "                                   weights,\n",
        "                                   search_k=True,\n",
        "                                   top_k_image = similar_images_emb.to(\"cuda\", dtype=torch.float16),\n",
        "                                   top_k_w =top_k_w)\n",
        "\n",
        "\"\"\"\n",
        "변수 설명\n",
        "image_text : 이미지+텍스트 컨디션 리스트\n",
        "weight : 이미지+텍스트 컨디션 가중치\n",
        "search_k : 최근접 이미지 k를 삽입할 것인지 여부 (bool)\n",
        "top_k_image : 찾은 최근접 이미지 벡터\n",
        "top_k_w : 최근접 이미지 가중치\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "new_image_add = pipe(prompt, **prior_out, height=768, width=768, num_images_per_prompt=num_images_per_prompt).images\n",
        "#prior_out은 image_embs + negative image embs로 구성됨\n",
        "# 여기서도  매개변수 지정 가능\n",
        "\"\"\"\n",
        "num_images_per_prompt : 생성할 이미지 개수\n",
        "guidance_scale : prompt 집중 정도\n",
        "\"\"\"\n",
        "if num_images_per_prompt > 1:\n",
        "  plt.figure(figsize=(10,5))\n",
        "  for i in range(len(new_image_add)):\n",
        "    plt.subplot(2, round(len(new_image_add)/2), i+1)\n",
        "    plt.imshow(new_image_add[i])\n",
        "    plt.axis('off')\n",
        "  plt.show()\n",
        "else:\n",
        "  plt.imshow(new_image_add)\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트 가중치\n",
        "query_text_w = 0.3\n",
        "cond_image_w = 0.6\n",
        "# 생성 이미지 개수\n",
        "num_images_per_prompt = 2\n",
        "\n",
        "# image_text 컨디션 리스트\n",
        "images_texts = [query_text] + input_images\n",
        "\n",
        "# 가중치 설정\n",
        "weights = [query_text_w] +[cond_image_w/(len(input_images)) for _ in input_images]\n",
        "# 이 뒷부분 [cond_image_w/(len(input_images)) for _ in input_images] 영역은\n",
        "# 이미지 개수에 맞게 직접 설정 가능 [0.3, 0.5] 이런 식\n",
        "\n",
        "# 검색 수행 X\n",
        "prompt = query_text\n",
        "prior_out = pipe_prior.interpolate(images_texts,\n",
        "                                   weights)\n",
        "\n",
        "\"\"\"\n",
        "변수 설명\n",
        "image_text : 이미지+텍스트 컨디션 리스트\n",
        "weight : 이미지+텍스트 컨디션 가중치\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "new_image_add = pipe(prompt, **prior_out, height=768, width=768, num_images_per_prompt=num_images_per_prompt).images\n",
        "#prior_out은 image_embs + negative image embs로 구성됨\n",
        "# 여기서도  매개변수 지정 가능\n",
        "\"\"\"\n",
        "num_images_per_prompt : 생성할 이미지 개수\n",
        "guidance_scale : prompt 집중 정도\n",
        "\"\"\"\n",
        "if num_images_per_prompt > 1:\n",
        "  plt.figure(figsize=(10,5))\n",
        "  for i in range(len(new_image_add)):\n",
        "    plt.subplot(2, round(len(new_image_add)/2), i+1)\n",
        "    plt.imshow(new_image_add[i])\n",
        "    plt.axis('off')\n",
        "  plt.show()\n",
        "else:\n",
        "  plt.imshow(new_image_add)\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "VLOTWUg-YjZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_text"
      ],
      "metadata": {
        "id": "_fbqzZCqFcQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nazyKsM_znUW"
      },
      "outputs": [],
      "source": [
        "# 생성 이미지 개수\n",
        "num_images_per_prompt = 2\n",
        "\n",
        "prompt = query_text\n",
        "\n",
        "negative_prompt = \"low quality, bad quality\" # negative 프롬프트 포함은 선택\n",
        "image_embeds, negative_image_embeds = pipe_prior(prompt, negative_prompt).to_tuple()\n",
        "\n",
        "new_image_onlytext = pipe(prompt, image_embeds=image_embeds, negative_image_embeds=negative_image_embeds, height=768, width=768, num_images_per_prompt=num_images_per_prompt).images\n",
        "\n",
        "if num_images_per_prompt > 1:\n",
        "  plt.figure(figsize=(10,5))\n",
        "  for i in range(len(new_image_onlytext)):\n",
        "    plt.subplot(2, round(len(new_image_onlytext)/2), i+1)\n",
        "    plt.imshow(new_image_onlytext[i])\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "else:\n",
        "  plt.imshow(new_image_onlytext)\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hY8KGa3L3vUy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2mOERaQ3vpW"
      },
      "source": [
        "## 성능 평가\n",
        "- 수정 X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y_fsiCY3xwT"
      },
      "outputs": [],
      "source": [
        "# CLIP Score - text\n",
        "\n",
        "model_name = \"openai/clip-vit-large-patch14\"\n",
        "clip_model = CLIPModel.from_pretrained(model_name)\n",
        "processor = CLIPProcessor.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# 텍스트와 생성 이미지 사이 유사도 평균을 반환\n",
        "def c_clip_score_text(images, prompt):\n",
        "    # 생성 이미지를 변환\n",
        "    image_list = [image.convert(\"RGB\") for image in images ]\n",
        "\n",
        "    # CLIP Processor를 사용하여 입력 변환\n",
        "    text_inputs = processor(text=[prompt], return_tensors=\"pt\", padding=True)\n",
        "    image_inputs = processor(images=image_list, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        text_emb = clip_model.get_text_features(**text_inputs)\n",
        "        image_emb = clip_model.get_image_features(**image_inputs)  # 이미지-텍스트 유사도 점수\n",
        "\n",
        "\n",
        "    # 정규화\n",
        "    text_emb =  torch.nn.functional.normalize(text_emb, p=2, dim=-1) # L2 정규화\n",
        "    image_emb =  torch.nn.functional.normalize(image_emb, p=2, dim=-1) # L2 정규화\n",
        "\n",
        "    # 유사도 계산을 위한 차원 확장\n",
        "    text_emb = text_emb.expand(image_emb.shape[0], -1)  # (n, 768)\n",
        "\n",
        "    # 코사인 유사도 계산\n",
        "    similarity = torch.nn.functional.cosine_similarity(text_emb, image_emb, dim=-1)\n",
        "\n",
        "    return similarity.mean()  # 평균\n",
        "\n",
        "# CLIP Score 비교\n",
        "clip_score_text = c_clip_score_text(new_image_onlytext, query_text)\n",
        "clip_score_added_cond = c_clip_score_text(new_image_add, query_text)\n",
        "\n",
        "print(f\"CLIP Score only text: {clip_score_text:.4f}\")\n",
        "print(f\"CLIP Score with image: {clip_score_added_cond:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CLIP Score - image\n",
        "\n",
        "# 입력 이미지 임베딩 벡터와 생성 이미지 사이의 유사도를 반환\n",
        "def c_clip_score_image(images, input_emb):\n",
        "\n",
        "    # 생성 이미지를 변환\n",
        "    image_list = [image.convert(\"RGB\") for image in images ]\n",
        "\n",
        "    # CLIP Processor를 사용하여 생성된 이미지를 변환\n",
        "    image_inputs_g = processor(images=image_list, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_emb = clip_model.get_image_features(**image_inputs_g) # 임베딩 벡터 생성\n",
        "\n",
        "\n",
        "    image_emb =  torch.nn.functional.normalize(image_emb, p=2, dim=-1) # L2 정규화\n",
        "\n",
        "    # 유사도 계산을 위한 차원 확장\n",
        "    input_emb = input_emb.expand(image_emb.shape[0], -1)  # (n, 768)\n",
        "\n",
        "    # 코사인 유사도 계산\n",
        "    similarity = torch.nn.functional.cosine_similarity(input_emb, image_emb, dim=-1)\n",
        "\n",
        "    return similarity.mean()  # 평균 반환\n",
        "\n",
        "# 입력 이미지 벡터로 변환\n",
        "image_inputs = processor(images=input_images, return_tensors=\"pt\", padding=True)\n",
        "with torch.no_grad():\n",
        "  image_embeddings = clip_model.get_image_features(**image_inputs)\n",
        "image_embeddings =  torch.nn.functional.normalize(image_embeddings, p=2, dim=-1) # L2 정규화\n",
        "\n",
        "# CLIP Score 비교\n",
        "for i, emb in enumerate(image_embeddings):\n",
        "  print(f'입력 이미지 경로 :  {uploaded_image_paths[i]}')\n",
        "  score = c_clip_score_image(new_image_add, emb)\n",
        "  print(f'CLIP Score with image : {score:.4f}')\n",
        "\n",
        "  score = c_clip_score_image(new_image_onlytext, emb)\n",
        "  print(f'CLIP Score only text: {score:.4f}')"
      ],
      "metadata": {
        "id": "yCL88lZ_Br6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded_image_paths"
      ],
      "metadata": {
        "id": "jLH-VfSoFrwD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}